{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init(\"C:\\Spark\")\n",
    "import os\n",
    "import sys\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import format_number\n",
    "from pyspark.sql.functions import max, min\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.functions import corr\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('walmart_stock').getOrCreate()\n",
    "sc = spark.sparkContext # Lancement de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2ème variante du lancement de la session Spark\n",
    "spark=SparkSession.builder\\\n",
    "                .master(\"local[*]\")\\\n",
    "                .appName(\"walmark_stock\")\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Load the Walmart Stock CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
      "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
      "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
      "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
      "|2012-01-13|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|\n",
      "|2012-01-17|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|\n",
      "|2012-01-18|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|\n",
      "|2012-01-19|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|\n",
      "|2012-01-20|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|\n",
      "|2012-01-23|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|\n",
      "|2012-01-24|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|\n",
      "|2012-01-25|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|\n",
      "|2012-01-26|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|\n",
      "|2012-01-27|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|\n",
      "|2012-01-30|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|\n",
      "|2012-01-31|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.option(\"header\", True).csv(r\"C:\\Users\\rbamb\\Downloads\\Cours M2 SEP\\CM Outils BIG DATA\\Outils Big Data\\walmart_stock.csv\")\n",
    "df.show(3) # On charge la base de données et on lance les trois premières lignes pour être sûre que tout fonctionne\n",
    "\n",
    "df.createOrReplaceTempView('Table')\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM Table\"\"\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) What are the column names ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les noms des colonnes sont : ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Les noms des colonnes sont :\", df.columns) # On va ressortir les noms de colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) What does the Schema look like ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Create a new dataframe with a column called HV_Ratio that is the ratio of the High Priceversus volume of stock traded for a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV_Ratio|\n",
      "+--------------------+\n",
      "|4.819714653321546E-6|\n",
      "|6.290848613094555E-6|\n",
      "|4.669412994783916E-6|\n",
      "|7.367338463826307E-6|\n",
      "|8.915604778943901E-6|\n",
      "|8.644477436914568E-6|\n",
      "|9.351828421515645E-6|\n",
      "| 8.29141562102703E-6|\n",
      "|7.712212102001476E-6|\n",
      "|7.071764823529412E-6|\n",
      "|1.015495466386981E-5|\n",
      "|6.576354146362592...|\n",
      "| 5.90145296180676E-6|\n",
      "|8.547679455011844E-6|\n",
      "|8.420709512685392E-6|\n",
      "|1.041448341728929...|\n",
      "|8.316075414862431E-6|\n",
      "|9.721183814992126E-6|\n",
      "|8.029436027707578E-6|\n",
      "|6.307432259386365E-6|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|            HV_Ratio|\n",
      "+--------------------+\n",
      "|4.819714653321546E-6|\n",
      "|6.290848613094555E-6|\n",
      "|4.669412994783916E-6|\n",
      "|7.367338463826307E-6|\n",
      "|8.915604778943901E-6|\n",
      "|8.644477436914568E-6|\n",
      "|9.351828421515645E-6|\n",
      "| 8.29141562102703E-6|\n",
      "|7.712212102001476E-6|\n",
      "|7.071764823529412E-6|\n",
      "|1.015495466386981E-5|\n",
      "|6.576354146362592...|\n",
      "| 5.90145296180676E-6|\n",
      "|8.547679455011844E-6|\n",
      "|8.420709512685392E-6|\n",
      "|1.041448341728929...|\n",
      "|8.316075414862431E-6|\n",
      "|9.721183814992126E-6|\n",
      "|8.029436027707578E-6|\n",
      "|6.307432259386365E-6|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_ratio = df.withColumn('HV_Ratio', df['High']/df['Volume']).select(['HV_Ratio'])\n",
    "df_ratio.show()\n",
    "\n",
    "#### SQL method\n",
    "spark.sql(\"\"\"SELECT High/Volume as HV_Ratio FROM Table\"\"\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) What day had the Peak High in Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jour avec le prix le plus élevé est le : 2015-01-13\n",
      "+-----+\n",
      "| Prix|\n",
      "+-----+\n",
      "|90.97|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(u\"Le jour avec le prix le plus élevé est le :\", df.orderBy(df['High'].desc()).select(['Date']).head(1)[0]['Date'])\n",
    "\n",
    "# SQL method\n",
    "spark.sql(\"\"\"SELECT ROUND(MAX(High), 2 ) as Prix FROM Table\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) What is the mean of the Close column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n",
      "+-------+\n",
      "|Moyenne|\n",
      "+-------+\n",
      "|  72.39|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(mean('Close')).show()\n",
    "\n",
    "# SQL method\n",
    "spark.sql(\"\"\"SELECT ROUND(MEAN(Close), 2) as Moyenne FROM Table\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) What is the max and min of the Volume column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the max and min of the Volume column?\n",
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|    9994400|   10010500|\n",
      "+-----------+-----------+\n",
      "\n",
      "+-------+--------+\n",
      "|    MAX|     MIN|\n",
      "+-------+--------+\n",
      "|9994400|10010500|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"What is the max and min of the Volume column?\")\n",
    "df.select(max('Volume'),min('Volume')).show()\n",
    "\n",
    "# SQL method\n",
    "spark.sql(\"\"\"SELECT MAX(Volume) as MAX, MIN(Volume) as MIN FROM Table\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9) How many days was the Close lower than 60 dollars?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y avait 81 jours durant lesquels la valeur Close était inférieur à 60.\n",
      "+-----+\n",
      "|Jours|\n",
      "+-----+\n",
      "|   81|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "days = df.filter(df['Close'] < 60).count()\n",
    "print(\"Il y avait\", days ,\"jours durant lesquels la valeur Close était inférieur à 60.\")\n",
    "\n",
    "# SQL method\n",
    "spark.sql(\"\"\"SELECT COUNT(Date) as Jours FROM Table WHERE Close < 60\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) What percentage of the time was the High greater than 80 dollars ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y avait 8.43 % de temps durant lequel la valeur High était supérieur à 80 dollars.\n",
      "+--------+\n",
      "|Max_High|\n",
      "+--------+\n",
      "|    8.43|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "percent = df.filter('High > 80').count() * 100/df.count()\n",
    "print(\"Il y avait\", round(percent, 2), \"% de temps durant lequel la valeur High était supérieur à 80 dollars.\")\n",
    "\n",
    "# SQL method\n",
    "spark.sql(\"\"\"SELECT ROUND((SELECT COUNT(High) FROM Table WHERE High > 80)*100/COUNT(High), 2) as Max_High FROM Table\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11) What is the max High per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Year|Max_High|\n",
      "+----+--------+\n",
      "|2012|    77.6|\n",
      "|2013|   81.37|\n",
      "|2014|   88.09|\n",
      "|2015|   90.97|\n",
      "|2016|   75.19|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SQL method\n",
    "spark.sql(\"\"\"SELECT YEAR(Date) as Year, ROUND(MAX(High), 2) as Max_High FROM Table GROUP BY Year ORDER BY Year\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12) What is the average Close for each Calendar Month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|Month|AVG_Close|\n",
      "+-----+---------+\n",
      "|    1|    71.45|\n",
      "|    2|    71.31|\n",
      "|    3|    71.78|\n",
      "|    4|    72.97|\n",
      "|    5|    72.31|\n",
      "|    6|     72.5|\n",
      "|    7|    74.44|\n",
      "|    8|    73.03|\n",
      "|    9|    72.18|\n",
      "|   10|    71.58|\n",
      "|   11|    72.11|\n",
      "|   12|    72.85|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SQL method\n",
    "spark.sql(\"\"\"SELECT MONTH(Date) as Month, ROUND(AVG(Close), 2) as AVG_Close FROM Table GROUP BY MONTH(Date) ORDER BY MONTH(Date)\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
